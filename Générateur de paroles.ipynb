{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy_deyqj3Ds_"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_04_text_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UZz30_993DtH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "import pandas \n",
    "import  eng_to_ipa as phon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2iZmlAuK3DtI",
    "outputId": "ddd4b502-5317-43a7-99d1-8ed4cc3f2869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#voir Beatles Analysis pour la source de ces textes\n",
    "\n",
    "with open('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/last_words.txt', 'r') as file:\n",
    "    last_words = file.read()\n",
    "with open('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/lyricsnoskipline.txt', 'r') as file: # sans skiplines\n",
    "    fulltext2 = file.read()  \n",
    "with open('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/fullchords.txt', 'r') as file:\n",
    "    fullchords = file.read() \n",
    "with open('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/lyricsskipline.txt', 'r') as file: #avec skiplines\n",
    "    fulltext = file.read() \n",
    "    \n",
    "    \n",
    "len(fulltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tpZycozp3DtI",
    "outputId": "ecf53964-5e94-4b51-c9a9-20400208c4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2236\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(fulltext)\n",
    "vocab = set()\n",
    "tokenized_text = []\n",
    "\n",
    "for token in doc:\n",
    "    word = ''.join([i for i in token.text])\n",
    "    word = word.strip()\n",
    "    vocab.add(word)\n",
    "    tokenized_text.append(word)\n",
    "        \n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0vL61JQ3DtJ",
    "outputId": "cec25166-6a98-493b-9e5f-acd6d6a97754",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h67R7HDh3DtK"
   },
   "outputs": [],
   "source": [
    "word2idx = dict((n, v) for v, n in enumerate(vocab))\n",
    "idx2word = dict((n, v) for n, v in enumerate(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tYst2mh63DtL"
   },
   "outputs": [],
   "source": [
    "tokenized_text = [word2idx[word] for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pTFmIz8k3DtL",
    "outputId": "7b4f08f0-9bab-49a8-ccf3-09f0b94963c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cwODk25L3DtM",
    "outputId": "2402c3fa-85d8-4df3-c8a7-735acc235ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 10418\n"
     ]
    }
   ],
   "source": [
    "# séparer le texte en séquence de longueur maxlen\n",
    "maxlen = 12\n",
    "step = 4\n",
    "sentences = []\n",
    "next_words = []\n",
    "for i in range(0, len(tokenized_text) - maxlen, step):\n",
    "    sentences.append(tokenized_text[i: i + maxlen])\n",
    "    next_words.append(tokenized_text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nyJPa6_d3DtP",
    "outputId": "49807ecc-6384-408e-9211-9423d468530e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10418 12\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences),len(sentences[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "<class 'numpy.ndarray'> <class 'list'> (10418, 12) (10418,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  53, 1152,   96, ..., 1017, 2014,  118])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on vectorize notre liste\n",
    "print ('Vectorization...')\n",
    "x=np.asarray(sentences)\n",
    "\n",
    "\n",
    "y=np.asarray(next_words, order='F')\n",
    "print (type(x), type(sentences), x.shape, y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour extraire la dernière syllabe\n",
    "\n",
    "def last_syl(phone):\n",
    "    vowels=['i', 'ɪ', 'e', 'ɛ', 'æ', 'a', 'ə', 'ɑ', 'ɒ', 'ɔ', 'ʌ', 'o', 'ʊ', 'u', 'y', 'ʏ', 'ø', 'œ', 'ɐ', 'ɜ', 'ɞ', 'ɘ', 'ɵ', 'ʉ', 'ɨ', 'ɤ', 'ɯ']   \n",
    "    n=1\n",
    "    while (n+1<len(phone)) and (phone[-n] not in vowels):\n",
    "        n+=1\n",
    "    if n+1<len(phone):\n",
    "        if phone[-(n+1)] in vowels:\n",
    "            n+=1\n",
    "    sy=re.sub('[!@#$.)\"*(]', '', a[-n:])\n",
    "    return sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mot</th>\n",
       "      <th>phonetic</th>\n",
       "      <th>syl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>whacking</td>\n",
       "      <td>ˈwækɪŋ</td>\n",
       "      <td>ɪŋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>spending</td>\n",
       "      <td>ˈspɛndɪŋ</td>\n",
       "      <td>ɪŋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>thirty</td>\n",
       "      <td>ˈθərˌdi</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kill</td>\n",
       "      <td>kɪl</td>\n",
       "      <td>ɪl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>2231</td>\n",
       "      <td>brew</td>\n",
       "      <td>bru</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2232</td>\n",
       "      <td>latest</td>\n",
       "      <td>ˈleɪtəst</td>\n",
       "      <td>əst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2233</td>\n",
       "      <td>benefit</td>\n",
       "      <td>ˈbɛnəfɪt</td>\n",
       "      <td>ɪt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2234</td>\n",
       "      <td>sea</td>\n",
       "      <td>si</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>2235</td>\n",
       "      <td>games</td>\n",
       "      <td>geɪmz</td>\n",
       "      <td>eɪmz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2236 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       mot  phonetic   syl\n",
       "0         0                          \n",
       "1         1  whacking    ˈwækɪŋ    ɪŋ\n",
       "2         2  spending  ˈspɛndɪŋ    ɪŋ\n",
       "3         3    thirty   ˈθərˌdi     i\n",
       "4         4      kill       kɪl    ɪl\n",
       "...     ...       ...       ...   ...\n",
       "2231   2231      brew       bru     u\n",
       "2232   2232    latest  ˈleɪtəst   əst\n",
       "2233   2233   benefit  ˈbɛnəfɪt    ɪt\n",
       "2234   2234       sea        si     i\n",
       "2235   2235     games     geɪmz  eɪmz\n",
       "\n",
       "[2236 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajouter la dimension de rimes au word embedding\n",
    "i=0\n",
    "wordidx=[]\n",
    "pword=[]\n",
    "psyl=[]\n",
    "v=list(vocab)\n",
    "while i<len(v):\n",
    "    a=phon.convert(v[i])\n",
    "    wordidx.append(word2idx[v[i]])\n",
    "    pword.append(a)\n",
    "    psyl.append(last_syl(a))\n",
    "    i+=1\n",
    "len(v), len(pword), len(psyl), len(wordidx)\n",
    "\n",
    "dfrhymes=pandas.DataFrame({'index': wordidx, 'mot':v, 'phonetic':pword, 'syl':psyl})\n",
    "dfrhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 505)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conversion du df en vecteur catégoriel\n",
    "rr = np.array(dfrhymes['syl'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "rr=le.fit_transform(rr)\n",
    "rr = to_categorical(rr)\n",
    "\n",
    "rr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importation du GLOVE Embedding...\n",
      "(2236, 300) 65 mots non trouvés dans GLOVE, pourcentage:  0.029069767441860465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.54377002, -0.040027  , -0.36065999, ..., -0.3608    ,\n",
       "         0.68027002, -0.63621002],\n",
       "       [-0.35767999,  0.22554   ,  0.43713999, ...,  0.45666   ,\n",
       "         0.30069   ,  0.071586  ],\n",
       "       ...,\n",
       "       [ 0.12173   , -0.52783   , -0.32065001, ...,  0.066712  ,\n",
       "        -0.07923   ,  0.20037   ],\n",
       "       [-0.49875   ,  0.45581999, -0.15914001, ..., -0.2325    ,\n",
       "         0.29359999,  0.25839999],\n",
       "       [-0.052904  , -0.52214998,  0.61027998, ...,  0.10879   ,\n",
       "         0.24323   ,  0.48824999]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Importation du GLOVE Embedding...')\n",
    "j=0\n",
    "embedding_vector = {}\n",
    "glove = open('C:/Users/benoi/Documents/HEC/_Hiver 2021/Machine Learning/projet/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in glove:\n",
    "    a = line.split(' ')\n",
    "    word = a[0]\n",
    "    coef = np.array(a[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocab),300))\n",
    "for word,i in word2idx.items():\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value\n",
    "    else:\n",
    "        j+=1\n",
    "        \n",
    "print(embedding_matrix.shape, j, 'mots non trouvés dans GLOVE, pourcentage: ', j/embedding_matrix.shape[0])\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_rhymes= np.concatenate((embedding_matrix,rr),axis=1)#on combine les deux matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 805)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_rhymes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "J2Awn22Z3DtR",
    "outputId": "daee07d0-b0f6-49ed-8d16-c6b363ec11c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10418, 12)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "34giyk0Q3DtR",
    "outputId": "650933bb-c4dd-4c0f-b099-d5095d5b309e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10418,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoBQiu5x3DtR"
   },
   "source": [
    "Because we encoded the $y$ value o dummy variables we can see, there are 6,418 elements for each row of $y$.  This large number of elements is because there are 6,418 words in the vocabulary.  Large vocabularies can significantly increase the amount of memory needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "efqFnEsJ3DtS",
    "outputId": "242b3ca2-44b7-4e06-b3fa-2347a1dbb5dd"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 805)           1799980   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 12, 512)           2174976   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 12, 256)           656384    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2236)              574652    \n",
      "=================================================================\n",
      "Total params: 5,600,232\n",
      "Trainable params: 3,800,252\n",
      "Non-trainable params: 1,799,980\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), 805, input_length=maxlen,  weights = [embedding_matrix_rhymes], trainable = False ))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.5,  recurrent_dropout=0.5)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.5)))\n",
    "model.add(Dense(len(vocab), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 805)           1799980   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 12, 512)           2174976   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 12, 256)           656384    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2236)              574652    \n",
      "=================================================================\n",
      "Total params: 5,600,232\n",
      "Trainable params: 5,600,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 146s 968ms/step - loss: 7.5182 - accuracy: 0.0940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af50bc5ac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### apprentissage\n",
    "model.fit(x, y,\n",
    "         batch_size=100,\n",
    "          epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/MODEL\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "kev9eNbS3DtT",
    "outputId": "eac55c2c-5265-4560-b3f9-95eea7aad45b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "105/105 [==============================] - 288s 3s/step - loss: 0.2347 - accuracy: 0.9304\n",
      "Epoch 2/3\n",
      "105/105 [==============================] - 288s 3s/step - loss: 0.2449 - accuracy: 0.9292\n",
      "Epoch 3/3\n",
      "105/105 [==============================] - 304s 3s/step - loss: 0.2347 - accuracy: 0.9319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b8e08f8348>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "         batch_size=100,\n",
    "          epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - 81s 1s/step - loss: 0.3897 - accuracy: 0.8973\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 82s 2s/step - loss: 0.3921 - accuracy: 0.8975\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3866 - accuracy: 0.9019\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3976 - accuracy: 0.8983\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 81s 2s/step - loss: 0.3659 - accuracy: 0.9085\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 81s 2s/step - loss: 0.3797 - accuracy: 0.9042\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3657 - accuracy: 0.9033\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3649 - accuracy: 0.9051\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3621 - accuracy: 0.9088\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3622 - accuracy: 0.9053\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3710 - accuracy: 0.9021\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3582 - accuracy: 0.9055\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 84s 2s/step - loss: 0.3589 - accuracy: 0.9072\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 81s 2s/step - loss: 0.3492 - accuracy: 0.9084\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.3454 - accuracy: 0.9074\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3381 - accuracy: 0.9128\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 82s 2s/step - loss: 0.3285 - accuracy: 0.9139\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 80s 2s/step - loss: 0.3268 - accuracy: 0.9159\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 80s 2s/step - loss: 0.3159 - accuracy: 0.9197\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 89s 2s/step - loss: 0.3159 - accuracy: 0.9181\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3234 - accuracy: 0.9142\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 81s 2s/step - loss: 0.3046 - accuracy: 0.9205\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 80s 2s/step - loss: 0.3045 - accuracy: 0.9217\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.3037 - accuracy: 0.9190\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 82s 2s/step - loss: 0.2925 - accuracy: 0.9260\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2808 - accuracy: 0.9274\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2961 - accuracy: 0.9201\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2930 - accuracy: 0.9247\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2796 - accuracy: 0.9262\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2832 - accuracy: 0.9262\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2708 - accuracy: 0.9332\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2759 - accuracy: 0.9297\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2590 - accuracy: 0.9346\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2641 - accuracy: 0.9294\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2704 - accuracy: 0.9284\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2589 - accuracy: 0.9303\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2555 - accuracy: 0.9337\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2537 - accuracy: 0.9363\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2631 - accuracy: 0.9292\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2569 - accuracy: 0.9320\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2528 - accuracy: 0.9328\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2548 - accuracy: 0.9323\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 77s 1s/step - loss: 0.2539 - accuracy: 0.9345\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 78s 1s/step - loss: 0.2544 - accuracy: 0.9326\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 76s 1s/step - loss: 0.2591 - accuracy: 0.9286\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 87s 2s/step - loss: 0.2319 - accuracy: 0.9391\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 83s 2s/step - loss: 0.2422 - accuracy: 0.9358\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 80s 2s/step - loss: 0.2376 - accuracy: 0.9356\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 79s 1s/step - loss: 0.2331 - accuracy: 0.9342\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 81s 2s/step - loss: 0.2370 - accuracy: 0.9369\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(x, y,\n",
    "         batch_size=200,\n",
    "          epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7684776186943054,\n",
       " 0.7726051211357117,\n",
       " 0.7779804468154907,\n",
       " 0.7799961566925049,\n",
       " 0.774716854095459,\n",
       " 0.789402961730957,\n",
       " 0.7929545044898987,\n",
       " 0.7916106581687927,\n",
       " 0.798713743686676,\n",
       " 0.801113486289978,\n",
       " 0.8058168292045593,\n",
       " 0.8119600415229797,\n",
       " 0.8048569560050964,\n",
       " 0.8174313902854919,\n",
       " 0.8166634440422058,\n",
       " 0.8200230598449707,\n",
       " 0.8261662721633911,\n",
       " 0.8270301222801208,\n",
       " 0.8228066563606262,\n",
       " 0.830773651599884,\n",
       " 0.8355730175971985,\n",
       " 0.835093080997467,\n",
       " 0.8397964835166931,\n",
       " 0.8489153385162354,\n",
       " 0.8451718091964722,\n",
       " 0.846419632434845,\n",
       " 0.858802080154419,\n",
       " 0.8539066910743713,\n",
       " 0.8542906641960144,\n",
       " 0.8576502203941345,\n",
       " 0.8583221435546875,\n",
       " 0.8552505373954773,\n",
       " 0.8667690753936768,\n",
       " 0.8711844682693481,\n",
       " 0.8710885047912598,\n",
       " 0.8704165816307068,\n",
       " 0.8764638304710388,\n",
       " 0.8742560744285583,\n",
       " 0.8828949928283691,\n",
       " 0.8773276805877686,\n",
       " 0.8770397305488586,\n",
       " 0.8806872963905334,\n",
       " 0.8848147392272949,\n",
       " 0.8840468525886536,\n",
       " 0.8875983953475952,\n",
       " 0.8851026892662048,\n",
       " 0.8892301917076111,\n",
       " 0.8900940418243408,\n",
       " 0.8926857113838196,\n",
       " 0.890861988067627]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_historique+=history2.history['accuracy']\n",
    "len(precision_historique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision_historique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-faa631abb91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_historique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Précision de notre modèle en fonction du nombre d'itérations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_historique' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig,ax =plt.subplots()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.plot(precision_historique)\n",
    "\n",
    "plt.title(\"Précision de notre modèle en fonction du nombre d'itérations\")\n",
    "plt.ylabel('Précision')\n",
    "plt.xlabel('Itérations')\n",
    "plt.show()\n",
    "#fig.savefig(\"nlp1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "g9BRkw2M3DtT"
   },
   "outputs": [],
   "source": [
    "#fonction trouvé sur https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_04_text_nlp.ipynb\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='I love the'\n",
    "b='If you are'\n",
    "c='The way you'\n",
    "d='Do you know'\n",
    "e='If I'\n",
    "f=\"I do n't think you should\"\n",
    "g='Why'\n",
    "h='What do you'\n",
    "i='I'   \n",
    "j='endofsong'\n",
    "k='She'\n",
    "text_starters=[a,b,c,d,e,f,g,h,i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Paroles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-10 19:34:26.252085</td>\n",
       "      <td>0.4</td>\n",
       "      <td>endofsong endofsong endofsong bird trust there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-10 19:44:27.033338</td>\n",
       "      <td>0.8</td>\n",
       "      <td>endofsong \\n stream trust there there all all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-10 19:54:08.168835</td>\n",
       "      <td>1.2</td>\n",
       "      <td>endofsong endofsong endofsong m'bop act toweri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-10 20:04:48.824885</td>\n",
       "      <td>0.4</td>\n",
       "      <td>I do n't think you should ran there there to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-10 20:15:54.324445</td>\n",
       "      <td>0.8</td>\n",
       "      <td>I do n't think you should ran there there to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-10 20:26:44.361215</td>\n",
       "      <td>1.2</td>\n",
       "      <td>I do n't think you should ran there there ca n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-10 20:37:20.505270</td>\n",
       "      <td>0.4</td>\n",
       "      <td>If you are endofsong endofsong act there there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-10 20:48:58.893926</td>\n",
       "      <td>0.8</td>\n",
       "      <td>If you are endofsong there there there long co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-10 21:04:01.417239</td>\n",
       "      <td>1.2</td>\n",
       "      <td>If you are now endofsong endofsong endofsong b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-10 21:18:18.842752</td>\n",
       "      <td>0.4</td>\n",
       "      <td>endofsong endofsong endofsong bird act there t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-04-10 21:40:14.348548</td>\n",
       "      <td>0.8</td>\n",
       "      <td>endofsong endofsong endofsong endofsong darn i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-04-10 21:45:10.092226</td>\n",
       "      <td>1.2</td>\n",
       "      <td>endofsong yeah ooh yeah endofsong endofsong en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-04-10 21:50:05.920059</td>\n",
       "      <td>0.4</td>\n",
       "      <td>I endofsong endofsong \\n there there there the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-04-10 21:55:31.941836</td>\n",
       "      <td>0.8</td>\n",
       "      <td>I endofsong \\n trust there there there for you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-04-10 22:00:54.970705</td>\n",
       "      <td>1.2</td>\n",
       "      <td>I \\n game there there all you all all \\n if yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-04-10 22:06:14.455094</td>\n",
       "      <td>0.4</td>\n",
       "      <td>If only I there there there there there all ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-10 22:12:24.255455</td>\n",
       "      <td>0.8</td>\n",
       "      <td>If only I there there there for you all all he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-04-10 22:18:30.394072</td>\n",
       "      <td>1.2</td>\n",
       "      <td>If only I there love \\n there 's only a man \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-04-10 22:24:31.657919</td>\n",
       "      <td>0.4</td>\n",
       "      <td>I endofsong endofsong endofsong trust there th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-10 22:30:22.877919</td>\n",
       "      <td>0.8</td>\n",
       "      <td>I endofsong endofsong ensemble there there the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-04-10 22:37:05.658539</td>\n",
       "      <td>1.2</td>\n",
       "      <td>I endofsong darn there there there need for yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-04-10 22:43:19.591941</td>\n",
       "      <td>0.4</td>\n",
       "      <td>endofsong endofsong endofsong bird trust there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-04-10 22:49:24.808108</td>\n",
       "      <td>0.8</td>\n",
       "      <td>endofsong endofsong endofsong bird act there t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-04-10 22:55:22.871130</td>\n",
       "      <td>1.2</td>\n",
       "      <td>endofsong endofsong endofsong there parted str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-04-10 23:01:33.929965</td>\n",
       "      <td>0.4</td>\n",
       "      <td>endofsong endofsong endofsong endofsong trust ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-04-10 23:09:07.753150</td>\n",
       "      <td>0.8</td>\n",
       "      <td>endofsong \\n oh endofsong \\n bird local there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-04-10 23:15:58.686021</td>\n",
       "      <td>1.2</td>\n",
       "      <td>endofsong husband endofsong endofsong yeah end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-04-10 23:22:41.088971</td>\n",
       "      <td>0.4</td>\n",
       "      <td>If only I there there there there there need \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-04-10 23:29:31.949122</td>\n",
       "      <td>0.8</td>\n",
       "      <td>If only I there there there there there all ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-04-10 23:36:15.045606</td>\n",
       "      <td>1.2</td>\n",
       "      <td>If only I there love could \\n every money as i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Datetime  Temperature  \\\n",
       "0  2021-04-10 19:34:26.252085          0.4   \n",
       "1  2021-04-10 19:44:27.033338          0.8   \n",
       "2  2021-04-10 19:54:08.168835          1.2   \n",
       "3  2021-04-10 20:04:48.824885          0.4   \n",
       "4  2021-04-10 20:15:54.324445          0.8   \n",
       "5  2021-04-10 20:26:44.361215          1.2   \n",
       "6  2021-04-10 20:37:20.505270          0.4   \n",
       "7  2021-04-10 20:48:58.893926          0.8   \n",
       "8  2021-04-10 21:04:01.417239          1.2   \n",
       "9  2021-04-10 21:18:18.842752          0.4   \n",
       "10 2021-04-10 21:40:14.348548          0.8   \n",
       "11 2021-04-10 21:45:10.092226          1.2   \n",
       "12 2021-04-10 21:50:05.920059          0.4   \n",
       "13 2021-04-10 21:55:31.941836          0.8   \n",
       "14 2021-04-10 22:00:54.970705          1.2   \n",
       "15 2021-04-10 22:06:14.455094          0.4   \n",
       "16 2021-04-10 22:12:24.255455          0.8   \n",
       "17 2021-04-10 22:18:30.394072          1.2   \n",
       "18 2021-04-10 22:24:31.657919          0.4   \n",
       "19 2021-04-10 22:30:22.877919          0.8   \n",
       "20 2021-04-10 22:37:05.658539          1.2   \n",
       "21 2021-04-10 22:43:19.591941          0.4   \n",
       "22 2021-04-10 22:49:24.808108          0.8   \n",
       "23 2021-04-10 22:55:22.871130          1.2   \n",
       "24 2021-04-10 23:01:33.929965          0.4   \n",
       "25 2021-04-10 23:09:07.753150          0.8   \n",
       "26 2021-04-10 23:15:58.686021          1.2   \n",
       "27 2021-04-10 23:22:41.088971          0.4   \n",
       "28 2021-04-10 23:29:31.949122          0.8   \n",
       "29 2021-04-10 23:36:15.045606          1.2   \n",
       "\n",
       "                                              Paroles  \n",
       "0   endofsong endofsong endofsong bird trust there...  \n",
       "1   endofsong \\n stream trust there there all all ...  \n",
       "2   endofsong endofsong endofsong m'bop act toweri...  \n",
       "3   I do n't think you should ran there there to h...  \n",
       "4   I do n't think you should ran there there to g...  \n",
       "5   I do n't think you should ran there there ca n...  \n",
       "6   If you are endofsong endofsong act there there...  \n",
       "7   If you are endofsong there there there long co...  \n",
       "8   If you are now endofsong endofsong endofsong b...  \n",
       "9   endofsong endofsong endofsong bird act there t...  \n",
       "10  endofsong endofsong endofsong endofsong darn i...  \n",
       "11  endofsong yeah ooh yeah endofsong endofsong en...  \n",
       "12  I endofsong endofsong \\n there there there the...  \n",
       "13  I endofsong \\n trust there there there for you...  \n",
       "14  I \\n game there there all you all all \\n if yo...  \n",
       "15  If only I there there there there there all ne...  \n",
       "16  If only I there there there for you all all he...  \n",
       "17  If only I there love \\n there 's only a man \\n...  \n",
       "18  I endofsong endofsong endofsong trust there th...  \n",
       "19  I endofsong endofsong ensemble there there the...  \n",
       "20  I endofsong darn there there there need for yo...  \n",
       "21  endofsong endofsong endofsong bird trust there...  \n",
       "22  endofsong endofsong endofsong bird act there t...  \n",
       "23  endofsong endofsong endofsong there parted str...  \n",
       "24  endofsong endofsong endofsong endofsong trust ...  \n",
       "25  endofsong \\n oh endofsong \\n bird local there ...  \n",
       "26  endofsong husband endofsong endofsong yeah end...  \n",
       "27  If only I there there there there there need \\...  \n",
       "28  If only I there there there there there all ne...  \n",
       "29  If only I there love could \\n every money as i...  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensongs=pandas.DataFrame({'Datetime':time, 'Temperature': temper, 'Paroles':chansons})\n",
    "#gensongs.to_csv( 'C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/Songs_generated.txt', mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def writelyrics(seed_text, nb_words, temp=0.8):#fonction pour générer du texte\n",
    "    print(nb_words, 'mots générés à partir du texte: ', seed_text)\n",
    "    j=0\n",
    "    time=[]\n",
    "    temper=[]\n",
    "    chansons=[]\n",
    "   \n",
    "    while j < nb_words:\n",
    "        sentence = seed_text.lower()\n",
    "        sentence=sentence.split(' ')\n",
    "        i=0\n",
    "        token_list=[]\n",
    "        while i<len(sentence):\n",
    "            token_list.append(word2idx[sentence[i]])#dictionnaire pour convertir le mot en index\n",
    "            i+=1\n",
    "        token_list = pad_sequences([token_list],# on pad la séquence\n",
    "                     maxlen=maxlen,padding='pre')\n",
    "        a = model.predict(token_list)#prediction du mot suivant\n",
    "        a=a[0, :]\n",
    "        next_index = sample(a, temperature=temp)\n",
    "        next_word = idx2word[next_index]#conversion index en mot\n",
    "        print(next_word)\n",
    "        \n",
    "        seed_text += \" \" + next_word#on ajoute à la liste\n",
    "        j+=1\n",
    "        \n",
    "    seed_text+= 'skiplines'#on ajoute le texte dans un fichier csv avec l'heure et la température\n",
    "    a=seed_text.replace('skiplines', '\\n')\n",
    "    time.append(datetime.now())\n",
    "    temper.append(temp)\n",
    "    chansons.append(a)\n",
    "    gensongs=pandas.DataFrame({'Datetime':time, 'Temperature': temper, 'Paroles':chansons}) \n",
    "    gensongs.to_csv('C:/Users/benoi/Documents/HEC/_Hiver 2021/Données textuelles/Songs_generated.txt',\n",
    "                    mode='a', header=False)  \n",
    "    bob=seed_text\n",
    "    print (bob)\n",
    "    print('----------------------------')\n",
    "    \n",
    "\n",
    "def build_df(nb_texts, nb_words):\n",
    "    i=0\n",
    "    while i<nb_texts:\n",
    "        #model.fit(x, y, batch_size=100, epochs=2, verbose=1)\n",
    "        r=random.randint(0, len(text_starters)-1)\n",
    "        s=text_starters[r]   \n",
    "        writelyrics(s, nb_words, temp=0.7)\n",
    "        writelyrics(s, nb_words, temp=1)\n",
    "        writelyrics(s, nb_words, temp=1.2)\n",
    "        i+=1   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def generatelyrics(seed_text, nb_words, temp=0.8, nb_lines=10):\n",
    "    print(nb_words, 'mots générés à partir du texte: ', seed_text)\n",
    "    j=0\n",
    "    while j < nb_words:\n",
    "        a = seed_text.lower()\n",
    "        sentence=a.split(' ')\n",
    "        i=0\n",
    "        token_list=[]\n",
    "        while i<len(sentence):\n",
    "            token_list.append(word2idx[sentence[i]])\n",
    "            i+=1\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=maxlen,padding='pre')\n",
    "        a = model.predict(token_list)\n",
    "        a=a[0, :]\n",
    "        next_index = sample(a, temperature=temp)\n",
    "        next_word = idx2word[next_index]\n",
    "        print(next_word)\n",
    "        seed_text += \" \" + next_word\n",
    "        j+=1\n",
    "\n",
    "        \n",
    "    bob=seed_text\n",
    "    print (bob)\n",
    "    print('----------------------------')  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_df(nb_texts, nb_words):\n",
    "    i=0\n",
    "    while i<nb_texts:\n",
    "        model.fit(x, y, batch_size=100, epochs=2, verbose=1)\n",
    "        r=random.randint(0, len(text_starters)-1)\n",
    "        s=text_starters[r]   \n",
    "        writelyrics(s, nb_words, temp=0.4)\n",
    "        writelyrics(s, nb_words, temp=0.8)\n",
    "        writelyrics(s, nb_words, temp=1.2)\n",
    "        i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjaA4eT03DtU"
   },
   "source": [
    "We will now fit the model. As the model fits, we display sample text that the model is generating.  We display text at several \"temperatures.\" For this example, temperature refers to the amount of randomness allowed in words chosen by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 mots générés à partir du texte:  If you are\n",
      "a\n",
      "dove\n",
      "with\n",
      "the\n",
      "bags\n",
      "but\n",
      "won\n",
      "skiplines\n",
      "but\n",
      "i\n",
      "told\n",
      "that\n",
      "we\n",
      "notice\n",
      "the\n",
      "end\n",
      "of\n",
      "90\n",
      "skiplines\n",
      "and\n",
      "i\n",
      "get\n",
      "home\n",
      "skiplines\n",
      "we\n",
      "'re\n",
      "on\n",
      "our\n",
      "way\n",
      "home\n",
      "skiplines\n",
      "we\n",
      "'re\n",
      "down\n",
      "and\n",
      "wondering\n",
      "from\n",
      "you\n",
      "and\n",
      "i\n",
      "'ll\n",
      "forgive\n",
      "you\n",
      "skiplines\n",
      "let\n",
      "me\n",
      "love\n",
      "me\n",
      "skiplines\n",
      "you\n",
      "If you are a dove with the bags but won skiplines but i told that we notice the end of 90 skiplines and i get home skiplines we 're on our way home skiplines we 're down and wondering from you and i 'll forgive you skiplines let me love me skiplines you\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "r=random.randint(0, len(text_starters)-1)\n",
    "s=text_starters[r] \n",
    "generatelyrics(s, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1209317f2809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'build_df' is not defined"
     ]
    }
   ],
   "source": [
    "build_df(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_df(8,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceci est un modele assez similaire, mais pour générer des accords. Nous l'avons copié à partir du même endroit que pour la fonction de sample, nous ne l'avons pas utilisé finalement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 158\n",
      "['wandering', 'leg', 'say', 'tonight']\n",
      "[0, 60, 87, 65, 118, 60, 87, 65, 118, 60, 87, 80, 118, 25, 141, 137, 40, 141, 130, 141, 137, 25, 141, 54, 141, 54, 141, 54, 141, 91, 94, 91, 124, 78, 130, 31, 130, 78, 124, 91, 94, 91, 141, 124, 91, 141, 124, 118, 25, 60, 91, 40, 60, 91, 40, 60, 118, 25, 60, 15, 136, 53, 40, 136, 118, 113, 83, 68, 107, 40, 76, 120, 40, 82, 76, 120, 155, 53, 135, 113, 83, 68, 107, 40, 82, 76, 120, 155, 53, 118, 25, 113, 155, 118, 50, 113, 78, 85, 108, 110, 113, 155, 118, 50, 113, 55, 85, 108, 110, 40, 91, 110, 25, 113, 107, 80, 117, 53, 50, 53, 91, 45, 113, 107, 113, 107, 80, 74, 117, 74, 53, 50, 53, 50, 14, 33, 137, 53, 151, 53, 33, 42, 33, 96, 4, 96, 33, 42, 92, 78, 14, 33, 110, 121, 110, 91, 94, 91, 80, 51, 108, 91, 94, 91, 80, 117, 80, 53, 50, 53, 91, 118, 25, 60, 58, 87, 40, 41, 126, 128, 60, 65, 87, 84, 40, 93, 87, 84, 40, 93, 87, 40, 118, 60, 93, 111, 93, 111, 93, 60, 77, 68, 36, 118, 60, 77, 68, 36, 118, 60, 77, 68, 36, 60, 77, 68, 36, 60, 77, 68, 36, 93, 111, 60, 77, 68, 36, 118, 60, 77, 68, 36, 118, 50, 53, 137, 40, 60, 50, 53, 137, 40, 60, 137, 50, 137, 25, 60, 118, 36, 91, 90, 60, 40, 60, 118, 60, 91, 40, 153, 21, 116, 45, 91, 60, 141, 91, 40, 153, 155, 116, 45, 91, 60, 91, 60, 28, 23, 60, 25, 113, 129, 50, 137, 113, 129, 50, 137, 118, 137, 113, 87, 113, 60, 118, 60, 80, 60, 80, 118, 60, 118, 60, 80, 60, 80, 118, 138, 62, 135, 60, 25, 68, 21, 91, 60, 91, 68, 126, 91, 29, 45, 91, 60, 23, 60, 23, 60, 94, 68, 126, 91, 29, 45, 91, 60, 25, 60, 40, 76, 120, 5, 60, 146, 68, 84, 40, 76, 120, 5, 60, 36, 68, 153, 40, 5, 76, 120, 40, 60, 80, 101, 74, 98, 60, 36, 146, 60, 21, 39, 72, 48, 91, 45, 94, 91, 60, 77, 68, 91, 94, 91, 60, 146, 36, 50, 60, 25, 113, 129, 107, 113, 129, 107, 129, 117, 56, 51, 39, 80, 96, 80, 91, 113, 91, 113, 107, 129, 107, 91, 94, 45, 94, 91, 45, 94, 45, 94, 80, 117, 33, 59, 80, 113, 129, 107, 129, 107, 118, 91, 113, 80, 56, 113, 107, 113, 25, 21, 87, 21, 87, 21, 87, 40, 118, 60, 80, 53, 80, 60, 25, 60, 15, 105, 15, 40, 2, 56, 40, 60, 17, 60, 15, 105, 15, 40, 2, 56, 40, 60, 17, 60, 91, 15, 80, 60, 15, 105, 15, 40, 60, 110, 121, 36, 40, 60, 110, 121, 36, 21, 80, 60, 118, 71, 118, 25, 60, 40, 137, 118, 60, 137, 37, 60, 136, 137, 25, 60, 36, 60, 36, 60, 36, 118, 135, 118, 135, 118, 135, 118, 106, 86, 128, 60, 36, 60, 36, 91, 94, 40, 120, 40, 80, 36, 118, 60, 36, 60, 25, 60, 113, 60, 113, 17, 60, 25, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 60, 118, 157, 87, 40, 120, 118, 135, 60, 118, 60, 118, 25, 141, 130, 91, 124, 91, 141, 60, 25, 60, 77, 68, 36, 91, 118, 60, 77, 68, 36, 91, 118, 30, 118, 87, 57, 91, 40, 56, 60, 91, 155, 118, 60, 91, 155, 118, 60, 118, 25, 60, 118, 91, 60, 118, 40, 25, 113, 22, 137, 96, 113, 22, 155, 91, 60, 21, 65, 155, 91, 60, 21, 65, 21, 118, 60, 21, 65, 21, 118, 60, 80, 118, 60, 21, 65, 21, 118, 60, 21, 65, 21, 118, 25, 113, 112, 113, 25, 60, 95, 40, 60, 95, 40, 60, 95, 40, 60, 95, 40, 60, 95, 40, 60, 137, 60, 137, 60, 96, 60, 40, 60, 40, 25, 33, 56, 43, 113, 33, 56, 43, 113, 80, 113, 80, 113, 80, 113, 80, 113, 25, 60, 80, 118, 53, 4, 113, 80, 118, 113, 40, 25, 60, 91, 60, 40, 60, 118, 60, 118, 87, 60, 118, 60, 118, 87, 60, 118, 60, 25, 60, 118, 60, 118, 60, 118, 87, 138, 40, 80, 60, 118, 87, 138, 40, 80, 60, 25, 113, 80, 91, 113, 80, 91, 113, 80, 91, 79, 75, 37, 137, 113, 25, 141, 40, 141, 136, 91, 141, 91, 60, 25, 113, 12, 113, 25, 60, 40, 60, 118, 87, 118, 60, 40, 93, 136, 60, 118, 60, 40, 93, 136, 60, 118, 60, 87, 118, 40, 60, 118, 25, 87, 40, 87, 40, 87, 118, 60, 40, 118, 25, 19, 91, 40, 87, 118, 40, 118, 113, 118, 25, 60, 3, 155, 68, 104, 102, 91, 102, 91, 60, 25, 60, 40, 60, 40, 87, 40, 60, 40, 87, 40, 87, 21, 118, 25, 157, 39, 157, 39, 155, 118, 157, 68, 21, 118, 113, 7, 113, 7, 103, 113, 7, 9, 116, 19, 113, 25, 155, 39, 155, 39, 152, 157, 60, 146, 154, 36, 155, 39, 155, 39, 152, 157, 60, 36, 102, 60, 141, 3, 78, 52, 78, 131, 52, 60, 141, 146, 3, 78, 40, 15, 40, 15, 60, 21, 73, 138, 147, 91, 40, 25, 60, 77, 87, 84, 87, 60, 77, 87, 84, 87, 155, 60, 128, 29, 118, 138, 73, 138, 91, 138, 73, 138, 91, 29, 60, 40, 118, 60, 40, 118, 60, 40, 45, 21, 39, 2, 118, 40, 25, 60, 118, 87, 2, 56, 60, 118, 40, 146, 155, 60, 25, 118, 121, 40, 80, 87, 118, 40, 60, 118, 121, 40, 80, 87, 84, 40, 118, 60, 77, 87, 84, 40, 118, 40, 60, 3, 149, 78, 14, 40, 78, 40, 41, 155, 60, 25, 60, 87, 155, 91, 60, 87, 65, 45, 94, 40, 94, 133, 146, 40, 94, 60, 87, 155, 91, 40, 65, 87, 21, 91, 60, 91, 40, 65, 87, 118, 91, 138, 44, 118, 25, 60, 73, 155, 90, 91, 60, 65, 155, 39, 65, 155, 120, 45, 91, 60, 73, 155, 65, 155, 120, 45, 91, 60, 48, 15, 4, 60, 76, 128, 76, 118, 60, 25, 156, 40, 60, 156, 60, 156, 40, 60, 156, 40, 60, 156, 40, 60, 146, 40, 110, 36, 60, 25, 60, 91, 40, 60, 87, 21, 39, 91, 60, 91, 40, 60, 87, 21, 39, 91, 60, 146, 36, 146, 91, 94, 91, 94, 91, 94, 91, 94, 60, 36, 60, 68, 36, 146, 60, 146, 36, 146, 91, 94, 91, 94, 91, 94, 91, 138, 44, 94, 60, 36, 60, 25, 60, 40, 60, 48, 118, 48, 93, 60, 50, 118, 21, 25, 118, 91, 118, 60, 40, 136, 40, 60, 40, 136, 40, 60, 40, 53, 136, 118, 91, 40, 60, 40, 110, 118, 25, 60, 120, 141, 60, 137, 141, 40, 60, 137, 141, 40, 25, 60, 40, 118, 91, 60, 40, 118, 91, 60, 91, 40, 33, 60, 80, 60, 118, 25, 60, 77, 68, 36, 138, 94, 30, 118, 45, 60, 77, 68, 36, 113, 68, 124, 90, 91, 90, 91, 60, 77, 68, 36, 138, 94, 30, 118, 45, 60, 77, 68, 36, 113, 68, 124, 90, 91, 40, 25, 15, 157, 93, 79, 118, 91, 40, 60, 77, 136, 40, 60, 77, 136, 40, 60, 77, 10, 112, 93, 60, 77, 112, 156, 136, 40, 118, 25, 60, 40, 60, 40, 60, 25, 63, 60, 102, 140, 123, 35, 60, 157, 61, 60, 25, 60, 40, 41, 155, 60, 40, 41, 155, 60, 118, 40, 60, 118, 40, 60, 40, 60, 25, 60, 91, 60, 91, 60, 91, 60, 91, 60, 40, 60, 40, 118, 91, 40, 25, 60, 155, 91, 87, 84, 2, 91, 60, 77, 87, 15, 40, 60, 155, 91, 87, 84, 40, 133, 155, 91, 60, 155, 91, 87, 84, 40, 133, 155, 91, 60, 25, 60, 87, 40, 118, 60, 91, 40, 60, 40, 60, 118, 25, 60, 118, 87, 40, 155, 64, 155, 64, 60, 40, 60, 118, 60, 40, 112, 60, 118, 112, 118, 40, 25, 60, 91, 60, 91, 60, 25, 118, 60, 146, 36, 26, 155, 26, 118, 46, 11, 54, 141, 155, 41, 40, 127, 60, 146, 36, 26, 155, 26, 118, 46, 11, 54, 141, 155, 41, 40, 25, 137, 40, 60, 137, 40, 60, 136, 91, 112, 137, 60, 112, 137, 60, 25, 113, 78, 94, 80, 130, 53, 97, 91, 130, 53, 91, 118, 25, 113, 50, 113, 19, 9, 157, 131, 113, 19, 9, 157, 131, 113, 25, 60, 77, 60, 77, 60, 77, 141, 40, 41, 40, 41, 141, 77, 141, 118, 138, 40, 60, 25, 19, 53, 40, 50, 137, 128, 19, 33, 50, 143, 47, 103, 9, 121, 107, 91, 113, 34, 129, 81, 107, 156, 137, 93, 50, 137, 7, 148, 113, 25, 73, 128, 73, 155, 76, 118, 60, 36, 60, 155, 73, 128, 73, 155, 76, 118, 60, 36, 91, 134, 144, 87, 133, 87, 133, 87, 80, 117, 80, 91, 118, 60, 155, 40, 41, 155, 120, 118, 138, 73, 135, 60, 36, 60, 125, 40, 41, 155, 120, 118, 138, 73, 135, 60, 21, 25, 60, 40, 60, 25, 60, 120, 60, 87, 36, 9, 33, 104, 118, 30, 118, 30, 118, 30, 118, 30, 60, 19, 120, 60, 25, 60, 77, 91, 94, 21, 48, 21, 39, 72, 48, 91, 45, 94, 45, 60, 77, 68, 91, 106, 45, 91, 106, 45, 60, 36, 60, 110, 91, 94, 91, 94, 91, 155, 26, 91, 106, 60, 118, 60, 110, 87, 94, 40, 76, 60, 77, 60, 25, 60, 30, 70, 60, 40, 70, 40, 60, 30, 70, 60, 40, 70, 40, 118, 60, 25, 60, 40, 60, 136, 40, 118, 60, 40, 60, 40, 21, 87, 40, 60, 40, 91, 60, 40, 118, 40, 118, 60, 40, 118, 60, 40, 118, 60, 40, 118, 25, 113, 91, 113, 19, 80, 53, 91, 113, 91, 113, 19, 80, 113, 60, 58, 48, 91, 60, 58, 48, 91, 60, 113, 118, 113, 118, 113, 118, 113, 53, 25, 60, 40, 118, 60, 16, 21, 99, 65, 80, 9, 80, 25, 60, 62, 40, 91, 118, 25, 60, 128, 26, 155, 91, 60, 118, 155, 91, 60, 40, 60, 40, 60, 77, 68, 36, 40, 76, 120, 76, 60, 77, 68, 36, 118, 68, 18, 60, 40, 68, 18, 60, 40, 25, 60, 112, 60, 112, 60, 87, 60, 87, 40, 60, 25, 60, 40, 118, 60, 40, 118, 60, 40, 156, 36, 60, 40, 146, 155, 60, 118, 60, 25, 60, 118, 65, 24, 91, 118, 142, 91, 2, 91, 2, 91, 60, 19, 32, 129, 1, 130, 1, 130, 91, 19, 80, 25, 60, 137, 40, 91, 60, 87, 40, 118, 60, 118, 87, 33, 60, 137, 40, 91, 60, 87, 40, 118, 60, 118, 68, 33, 56, 80, 60, 91, 40, 60, 25, 60, 36, 129, 33, 42, 75, 49, 67, 72, 118, 72, 118, 60, 113, 91, 50, 91, 94, 113, 129, 107, 118, 128, 19, 9, 157, 9, 84, 117, 118, 25, 60, 155, 91, 40, 118, 155, 93, 91, 21, 91, 40, 60, 135, 118, 25, 60, 116, 65, 155, 91, 60, 116, 65, 155, 80, 60, 91, 40, 60, 91, 40, 60, 25, 60, 155, 65, 60, 40, 110, 30, 110, 87, 112, 118, 9, 80, 40, 80, 146, 91, 90, 60, 40, 146, 91, 90, 60, 110, 87, 118, 40, 118, 60, 110, 87, 118, 40, 91, 122, 15, 60, 25, 36, 118, 13, 142, 36, 60, 118, 19, 84, 19, 13, 140, 96, 19, 41, 37, 40, 25, 21, 96, 136, 40, 60, 21, 96, 136, 40, 60, 21, 41, 40, 60, 100, 91, 40, 60, 100, 91, 40, 60, 36, 91, 60, 25, 113, 27, 113, 38, 113, 27, 113, 27, 113, 27, 113, 27, 113, 27, 113, 27, 53, 104, 50, 108, 113, 27, 113, 38, 113, 27, 113, 25, 138, 76, 112, 136, 60, 138, 60, 136, 40, 60, 25, 60, 68, 65, 60, 68, 65, 118, 60, 68, 3, 6, 78, 142, 33, 60, 68, 65, 118, 60, 25, 89, 60, 91, 87, 91, 87, 91, 118, 104, 118, 60, 77, 68, 36, 40, 112, 60, 77, 68, 36, 40, 98, 60, 25, 40, 118, 40, 118, 40, 118, 91, 60, 40, 118, 60, 87, 40, 118, 60, 40, 25, 40, 136, 94, 60, 54, 46, 91, 40, 41, 91, 118, 68, 128, 87, 153, 84, 18, 40, 136, 135, 60, 54, 91, 40, 146, 91, 118, 128, 87, 153, 68, 18, 40, 136, 118, 25, 113, 83, 114, 50, 113, 137, 40, 118, 113, 83, 114, 78, 113, 137, 40, 118, 113, 83, 9, 68, 78, 113, 137, 53, 118, 25, 87, 60, 87, 21, 80, 60, 25, 60, 40, 60, 40, 60, 91, 40, 60, 118, 25, 60, 21, 40, 41, 155, 60, 65, 136, 135, 7, 60, 21, 118, 25, 21, 93, 91, 60, 65, 87, 40, 93, 60, 65, 87, 40, 93, 60, 21, 39, 137, 109, 118, 60, 141, 60, 25, 60, 138, 68, 79, 36, 138, 68, 112, 100, 112, 100, 33, 36, 138, 68, 79, 36, 138, 68, 153, 68, 153, 68, 153, 80, 51, 60, 40, 118, 60, 40, 118, 60, 40, 118, 25, 60, 21, 65, 40, 60, 21, 65, 40, 155, 91, 155, 91, 87, 21, 155, 91, 155, 91, 53, 113, 80, 91, 113, 80, 91, 21, 25, 68, 90, 91, 60, 77, 87, 25, 60, 40, 60, 91, 40, 60, 118, 25, 141, 40, 141, 91, 60, 25, 21, 80, 60, 118, 40, 60, 21, 80, 60, 25, 113, 78, 113, 78, 113, 72, 88, 155, 91, 78, 40, 78, 40, 78, 137, 25, 118, 115, 118, 91, 60, 40, 118, 91, 60, 40, 118, 91, 60, 40, 118, 91, 60, 40, 118, 25, 141, 96, 141, 96, 137, 96, 40, 91, 137, 96, 141, 113, 7, 9, 116, 19, 80, 150, 89, 33, 113, 80, 113, 141, 96, 141, 96, 137, 96, 40, 91, 137, 96, 60, 25, 66, 140, 40, 25, 60, 40, 60, 156, 40, 91, 50, 118, 25, 87, 84, 9, 42, 120, 20, 60, 87, 84, 9, 42, 120, 20, 60, 21, 93, 60, 87, 15, 93, 60, 25, 60, 156, 40, 60, 156, 40, 60, 87, 56, 87, 156, 60, 156, 40, 60, 25, 141, 40, 141, 91, 141, 46, 40, 112, 141, 91, 141, 60, 25, 60, 113, 40, 113, 155, 118, 113, 25, 60, 91, 87, 91, 155, 118, 60, 70, 60, 68, 118, 40, 118, 60, 25, 118, 60, 118, 29, 128, 60, 36, 87, 133, 40, 120, 118, 29, 128, 118, 60, 36, 60, 77, 87, 133, 87, 84, 40, 120, 40, 120, 118, 29, 128, 118, 60, 36, 60, 77, 87, 133, 87, 84, 40, 120, 40, 120, 40, 118, 60, 87, 118, 40, 118, 60, 36, 60, 77, 87, 133, 87, 84, 40, 120, 118, 128, 60, 36, 60, 25, 60, 17, 60, 17, 93, 60, 17, 60, 17, 93, 60, 40, 60, 40, 118, 113, 107, 113, 83, 50, 69, 118, 113, 83, 78, 113, 25, 60, 136, 40, 60, 136, 40, 60, 136, 40, 60, 136, 40, 60, 138, 76, 74, 36, 118, 60, 25, 60, 36, 40, 120, 60, 36, 40, 120, 60, 36, 40, 120, 60, 36, 60, 36, 60, 36, 40, 120, 60, 36, 40, 120, 60, 36, 40, 120, 40, 120, 40, 120, 40, 60, 36, 40, 120, 60, 36, 40, 120, 118, 135, 118, 135, 40, 120, 60, 36, 118, 135, 118, 135, 40, 120, 60, 36, 60, 25, 141, 91, 40, 91, 60, 25, 60, 124, 91, 60, 124, 91, 133, 145, 54, 60, 40, 135, 21, 91, 29, 118, 25, 60, 8, 146, 21, 87, 118, 60, 8, 146, 21, 87, 118, 68, 155, 60, 40, 118, 60, 8, 146, 21, 40, 60, 40, 60, 40, 60, 40, 60, 25, 19, 119, 33, 130, 139, 88, 78, 155, 118, 113, 19, 60, 91, 87, 91, 40, 91, 60, 15, 136, 118, 60, 80, 111, 33, 111, 60, 15, 60, 156, 62, 91, 60, 78, 62, 60, 9, 40, 62, 78, 9, 40, 91, 124, 60, 25, 50, 118, 113, 118, 50, 113, 118, 137, 21, 118, 113, 118, 137, 21, 118, 113, 53, 50, 135, 113, 83, 78, 107, 74, 135, 113, 83, 78, 107, 74, 118, 113, 118, 25, 132, 21, 118, 87, 40, 87, 144, 87, 118, 60, 25, 21, 39, 72, 48, 118, 138, 73, 135, 60, 146, 36, 146, 87, 133, 87, 40, 76, 120, 21, 48, 21, 93, 91, 21, 39, 72, 48, 118, 138, 73, 135, 60, 146, 36, 146, 87, 133, 87, 40, 76, 120, 118, 135, 118, 60, 68, 36, 146, 60, 87, 133, 112, 100, 112, 60, 36, 60, 36, 87, 133, 112, 100, 112, 60, 68, 36, 146, 60, 40, 60, 21, 118, 60, 25, 60, 87, 60, 87, 60, 87, 60, 91, 40, 87, 60, 91, 60, 87, 60, 87, 60, 91, 40, 87, 60, 91, 60, 91, 96, 118, 40, 91, 118, 25, 60, 65, 136, 24, 9, 19, 65, 136, 24, 9, 19, 40, 41, 155, 118, 60, 25, 19, 120, 117, 113, 91, 113, 19, 120, 117, 113, 91, 113, 53, 78, 53, 78, 53, 78, 53, 91, 113, 53, 21, 118, 60, 87, 21, 118, 60, 90, 118, 25, 60, 58, 87, 36, 40, 93, 60, 113, 91, 113, 91, 113, 30, 118, 25, 60, 118, 60, 118, 60, 118, 91, 118, 25, 60, 40, 60, 40, 60, 40, 112, 93, 60, 21, 39, 72, 48, 118, 138, 73, 135, 60, 77, 68, 36, 87, 110, 116, 133, 21, 39, 72, 48, 118, 138, 73, 135, 60, 72, 30, 118, 25, 60, 25, 60, 118, 65, 40, 118, 60, 137, 40, 118, 60, 137, 93, 118, 25, 60, 40, 60, 118, 60, 25]\n",
      "nb sequences: 1010\n",
      "Vectorization...\n",
      "(1010, 8, 158)\n",
      "(1010, 158)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_77 (LSTM)               (None, 8, 250)            409000    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 8, 250)            0         \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 250)               501000    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 158)               39658     \n",
      "=================================================================\n",
      "Total params: 949,658\n",
      "Trainable params: 949,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "60 113\n",
      "(1010, 8, 158) [[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(fullchords)\n",
    "vocabc = set()\n",
    "tokenized_chord = []\n",
    "\n",
    "for token in doc:\n",
    "    word = ''.join([i for i in token.text])\n",
    "    word = word.strip()\n",
    "    vocabc.add(word)\n",
    "    tokenized_chord.append(word)\n",
    "        \n",
    "print(f\"Vocab size: {len(vocabc)}\")\n",
    "\n",
    "print(list(vocab)[1:5])\n",
    "\n",
    "#We need an easy way to convert words into indexes and vise versa. The following code builds two such indexes.\n",
    "\n",
    "chord2idx = dict((n, v) for v, n in enumerate(vocabc))\n",
    "idx2chord = dict((n, v) for n, v in enumerate(vocabc))\n",
    "\n",
    "#We can now tokenize the text; this process replaces each word with the correct token.\n",
    "\n",
    "tokenized_chord = [chord2idx[word] for word in tokenized_chord]\n",
    "\n",
    "#If we display the tokenized text, we see an array of index values for each word.\n",
    "\n",
    "print(tokenized_chord)\n",
    "\n",
    "#Next, we break the tokenized text into sequences that are of consistent length.  It is necessary to specify this length; here we use a sequence length of 6.\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen words\n",
    "maxlen1 = 8\n",
    "step1 = 3\n",
    "sentences1 = []\n",
    "next_words1 = []\n",
    "for i in range(0, len(tokenized_chord) - maxlen1, step1):\n",
    "    sentences1.append(tokenized_chord[i: i + maxlen1])\n",
    "    next_words1.append(tokenized_chord[i + maxlen1])\n",
    "print('nb sequences:', len(sentences1))\n",
    "\n",
    "#We can display the first five sequences to get an idea of the appearance of the data.\n",
    "\n",
    "#sentences[0:5]\n",
    "\n",
    "#Finally, we create the $x$ and $y$ vectors.  The $x$ is a Numpy encoding of the tokenization that we just performed.  We use the first six elements of each tokenization to predict the seventh element.  We convert the next element to dummy variables, and it becomes the $y$.  For each of the sequences, we teach the neural network to predict the sixth element (or word) based on the previous five elements.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Vectorization...')\n",
    "x1 = np.zeros((len(sentences1), maxlen1, len(vocabc)), dtype=np.bool)\n",
    "y1 = np.zeros((len(sentences1), len(vocabc)), dtype=np.bool)\n",
    "for i, sentence1 in enumerate(sentences1):\n",
    "    for t, word1 in enumerate(sentence1):\n",
    "        x1[i, t, word1] = 1\n",
    "    y1[i, next_words1[i]] = 1\n",
    "\n",
    "#We display the shapes of the $x$ and $y$.\n",
    "\n",
    "print(x1.shape)\n",
    "\n",
    "print(y1.shape)\n",
    "\n",
    "\n",
    "y[0:5]\n",
    "\n",
    "\n",
    "#Now we can train an LSTM-based neural network to generate text.\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "modelc = Sequential()\n",
    "modelc.add(LSTM(250, input_shape=(maxlen1, len(vocabc)), return_sequences=True ))\n",
    "modelc.add(Dropout(0.3))\n",
    "modelc.add(LSTM(250, input_shape=(maxlen1, len(vocabc))))\n",
    "modelc.add(Dense(len(vocabc), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "modelc.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "\n",
    "print(modelc.summary())\n",
    "\n",
    "#This function collects sample generations from the neural network.  The temperature variable specifies how conservative, or less random, the predictions will be.  Higher temperatures encourage more \"creativity\" from the neural network; however, they also promote more nonsensical output.\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "#Keras calls the following function at the end of each training Epoch.  The code generates sample text generations that visually demonstrate the neural network better at text generation.  As the neural network trains, the generations should look more realistic.\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print(\"******************************************************\")\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(tokenized_chord) - 3)\n",
    "    for temperature in [0.6, 1, 1.2]:\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        #generated = ''\n",
    "        sentencet = tokenized_chord[start_index: start_index + 3]\n",
    "        #generated += sentencet\n",
    "        o = ' '.join([idx2chord[idx] for idx in sentencet])\n",
    "        print(f'----- Generating with seed: \"{o}\"')\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(20):\n",
    "            x_pred = np.zeros((1, maxlen1, len(vocabc)))\n",
    "            for t, word in enumerate(sentencet):\n",
    "                x_pred[0, t, word] = 1.\n",
    "                \n",
    "\n",
    "            preds = modelc.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = idx2chord[next_index]\n",
    "\n",
    "            #generated += next_char\n",
    "            sentencet = sentencet[1:]\n",
    "            sentencet.append(next_index)\n",
    "\n",
    "            sys.stdout.write(next_word)\n",
    "            sys.stdout.write(' ') \n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "#We will now fit the model. As the model fits, we display sample text that the model is generating.  We display text at several \"temperatures.\" For this example, temperature refers to the amount of randomness allowed in words chosen by the neural network.\n",
    "\n",
    "print (chord2idx['I'], chord2idx['i'])\n",
    "print (x1.shape, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "101/101 [==============================] - 25s 102ms/step - loss: 4.3212\n",
      "******************************************************\n",
      "----- Generating text after Epoch: 0\n",
      "----- temperature: 0.6\n",
      "----- Generating with seed: \"V7 i iv\"\n",
      "IV I ENDOFSONG IV IV IV V vi7 IV IV vi ENDOFSONG I IV V V V IV IV vi \n",
      "----- temperature: 1\n",
      "----- Generating with seed: \"V7 i iv\"\n",
      "IV I ENDOFSONG ii7 IV VI ii7 V7 iv64 IV6 VI64 IV42 V7 bIII V IV7 IV I I64 I42 \n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"V7 i iv\"\n",
      "V64 III bIII IV IV ii42 VI7 I bVI7 V7 VI43 IV V7 ii64 vii7 V7 I I6 I V43 \n",
      "Epoch 2/5\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 3.6351\n",
      "******************************************************\n",
      "----- Generating text after Epoch: 1\n",
      "----- temperature: 0.6\n",
      "----- Generating with seed: \"I V7 IV\"\n",
      "III IV ENDOFSONG V vi V IV IV I I IV I V vi vi7 V bVII V7 ENDOFSONG I \n",
      "----- temperature: 1\n",
      "----- Generating with seed: \"I V7 IV\"\n",
      "II bII I IV vi6 V43 bVI7 V65 ENDOFSONG i64 II V64 vi64 IV64 II ii iv I42 i II \n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"I V7 IV\"\n",
      "iv vi64 I V i42sus4 bIII VI7 ii42 vadd1 I6 V42 vii42 bI v42 V7 I9sus4 V7sus4 i i i \n",
      "Epoch 3/5\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 3.5518\n",
      "******************************************************\n",
      "----- Generating text after Epoch: 2\n",
      "----- temperature: 0.6\n",
      "----- Generating with seed: \"V7 vii6 bVII7\"\n",
      "ENDOFSONG iv V7 IV I I64 I I I I I I64 I V IV64 V7 I vi42 V vi \n",
      "----- temperature: 1\n",
      "----- Generating with seed: \"V7 vii6 bVII7\"\n",
      "vi7 V65 V7sus4 V9sus4 iii7 IV9 vii6 i v9 vii65 iv I ii43 III I9sus4 vi V6 II vi7 # \n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"V7 vii6 bVII7\"\n",
      "vii7 IV V ii7 iii VI ENDOFSONG V6 iiadd9 bII6 v65 I6 Vsus4 I6sus4 vadd1 vi Isus4 I IV bVII \n",
      "Epoch 4/5\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 3.4020\n",
      "******************************************************\n",
      "----- Generating text after Epoch: 3\n",
      "----- temperature: 0.6\n",
      "----- Generating with seed: \"ii bVII V7\"\n",
      "I IV V I V IV I I I I43 I bIII7 I42 iii bIII ii7 vi IV I I \n",
      "----- temperature: 1\n",
      "----- Generating with seed: \"ii bVII V7\"\n",
      "V7 i i64 V i vii42 iii IV7addsus21 vii42add1 IV I43sus4 IV V9sus4 vi43 V V7 bIII I ENDOFSONG bVI \n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"ii bVII V7\"\n",
      "bIII V43 V7 V V65 IV bVI bVI65 ENDOFSONG vi43 VII VII iv7 V7 V42 I I64 III vii I6 \n",
      "Epoch 5/5\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 3.3922\n",
      "******************************************************\n",
      "----- Generating text after Epoch: 4\n",
      "----- temperature: 0.6\n",
      "----- Generating with seed: \"IV V i\"\n",
      "I i iv7 VI VII VI7 i I V VII V V V I i V IV i I VI \n",
      "----- temperature: 1\n",
      "----- Generating with seed: \"IV V i\"\n",
      "vi65 i ii42 ii VI VII VI7 VII I V V7 ii7 V64 bIII ENDOFSONG bIII7 I42 iv I IV7 \n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"IV V i\"\n",
      "i7 V64 VII64 ii65 Iadd9 iii7 v I7 i6 V64 vi43 i ii7 V7 iii i vadd1 III VI42 vii7 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181103325c8>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "modelc.fit(x1, y1,\n",
    "          batch_size=10,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "def prepare_str()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(fullchords)\n",
    "    vocabc = set()\n",
    "    tokenized_chord = []\n",
    "\n",
    "    for token in doc:\n",
    "        word = ''.join([i for i in token.text])\n",
    "        word = word.strip()\n",
    "        vocabc.add(word)\n",
    "        tokenized_chord.append(word)\n",
    "\n",
    "    print(f\"Vocab size: {len(vocabc)}\")\n",
    "\n",
    "    print(list(vocab)[1:5])\n",
    "\n",
    "    #We need an easy way to convert words into indexes and vise versa. The following code builds two such indexes.\n",
    "\n",
    "    chord2idx = dict((n, v) for v, n in enumerate(vocabc))\n",
    "    idx2chord = dict((n, v) for n, v in enumerate(vocabc))\n",
    "\n",
    "    #We can now tokenize the text; this process replaces each word with the correct token.\n",
    "\n",
    "    tokenized_chord = [chord2idx[word] for word in tokenized_chord]\n",
    "\n",
    "    #If we display the tokenized text, we see an array of index values for each word.\n",
    "\n",
    "    print(tokenized_chord)\n",
    "\n",
    "    #Next, we break the tokenized text into sequences that are of consistent length.  It is necessary to specify this length; here we use a sequence length of 6.\n",
    "\n",
    "    # cut the text in semi-redundant sequences of maxlen words\n",
    "    maxlen1 = ml\n",
    "    step1 = s\n",
    "    sentences1 = []\n",
    "    next_words1 = []\n",
    "    for i in range(0, len(tokenized_chord) - maxlen1, step1):\n",
    "        sentences1.append(tokenized_chord[i: i + maxlen1])\n",
    "        next_words1.append(tokenized_chord[i + maxlen1])\n",
    "    print('nb sequences:', len(sentences1))\n",
    "\n",
    "    #We can display the first five sequences to get an idea of the appearance of the data.\n",
    "\n",
    "    #sentences[0:5]\n",
    "\n",
    "    #Finally, we create the $x$ and $y$ vectors.  The $x$ is a Numpy encoding of the tokenization that we just performed.  We use the first six elements of each tokenization to predict the seventh element.  We convert the next element to dummy variables, and it becomes the $y$.  For each of the sequences, we teach the neural network to predict the sixth element (or word) based on the previous five elements.\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    print('Vectorization...')\n",
    "    x1 = np.zeros((len(sentences1), maxlen1, len(vocabc)), dtype=np.bool)\n",
    "    y1 = np.zeros((len(sentences1), len(vocabc)), dtype=np.bool)\n",
    "    for i, sentence1 in enumerate(sentences1):\n",
    "        for t, word1 in enumerate(sentence1):\n",
    "            x1[i, t, word1] = 1\n",
    "        y1[i, next_words1[i]] = 1\n",
    "\n",
    "    #We display the shapes of the $x$ and $y$.\n",
    "\n",
    "    print(x1.shape)\n",
    "\n",
    "    print(y1.shape)\n",
    "\n",
    "\n",
    "    y[0:5]\n",
    "\n",
    "\n",
    "#Now we can train an LSTM-based neural network to generate text.\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "modelc = Sequential()\n",
    "modelc.add(LSTM(250, input_shape=(maxlen1, len(vocabc)), return_sequences=True ))\n",
    "modelc.add(Dropout(0.3))\n",
    "modelc.add(LSTM(250, input_shape=(maxlen1, len(vocabc))))\n",
    "modelc.add(Dense(len(vocabc), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "modelc.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "\n",
    "print(modelc.summary())\n",
    "\n",
    "#This function collects sample generations from the neural network.  The temperature variable specifies how conservative, or less random, the predictions will be.  Higher temperatures encourage more \"creativity\" from the neural network; however, they also promote more nonsensical output.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de t81_558_class_11_04_text_nlp.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_04_text_nlp.ipynb",
     "timestamp": 1616130119625
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
